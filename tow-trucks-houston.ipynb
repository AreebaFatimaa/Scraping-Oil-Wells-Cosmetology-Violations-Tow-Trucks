{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86a49bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screenshot saved as tdlr_search_page.png\n"
     ]
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "\n",
    "async def open_page():\n",
    "    async with async_playwright() as p:\n",
    "        # Launch Firefox headlessly\n",
    "        browser = await p.firefox.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        # Go to the TDLR search page\n",
    "        await page.goto(\"https://www.tdlr.texas.gov/tools_search/\", wait_until=\"networkidle\")\n",
    "        \n",
    "        # Take a screenshot\n",
    "        await page.screenshot(path=\"tdlr_search_page.png\")\n",
    "        print(\"Screenshot saved as tdlr_search_page.png\")\n",
    "        \n",
    "        await browser.close()\n",
    "\n",
    "# Run the async function\n",
    "await open_page()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6006395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=TargetClosedError('Target page, context or browser has been closed')>\n",
      "playwright._impl._errors.TargetClosedError: Target page, context or browser has been closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDLR search page loaded.\n"
     ]
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "\n",
    "# Your list of ZIP codes\n",
    "zip_codes = [\n",
    "    \"77002\",\"77021\",\"77040\",\"77059\",\"77078\",\"77098\",\"77384\",\"77479\",\"77547\",\n",
    "    \"77003\",\"77022\",\"77041\",\"77060\",\"77079\",\"77099\",\"77385\",\"77484\",\"77571\",\n",
    "    # (add all remaining ZIP codes here)\n",
    "]\n",
    "\n",
    "# Function to open the page and return the page object\n",
    "async def open_tdlr_page():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.firefox.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(\"https://www.tdlr.texas.gov/tools_search/\", wait_until=\"networkidle\")\n",
    "        return page, browser\n",
    "\n",
    "# Example usage: open page\n",
    "page, browser = await open_tdlr_page()\n",
    "print(\"TDLR search page loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a168014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   TDLR Tow Truck and Vehicle Storage Facility Inquiry\n",
      "  </title>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <meta content=\"Microsoft Visual Studio\" name=\"GENERATOR\"/>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <meta content=\"Welcome to the Tow Truck and Vehicle Storage Facility Inquiry Information Page. \n",
      "\t\t\t\tThis web application allows users to obtain information on companies that have \n",
      "\t\t\t\tobtained registration through TDLR. This includes addresses, insurance records, \n",
      "\t\t\t\trecent activities, and vehicle data.\" name=\"description\"/>\n",
      "  <meta content=\"Tow Trucks, Vehicle Storage Facility, registration, insurance, Permit \n",
      "\t\t\t\tRestrictions, Texas Department of Licensing and Regulation, TDLR\" name=\"keywords\"/>\n",
      "  <meta content=\"Transportation\" name=\"subject\"/>\n",
      "  <meta content=\"Programs and services\" name=\"type\"/>\n",
      "  <meta content=\"Texas Department of Licensing and Regulation (State of Texas)\" name=\"author\"/>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "\n",
    "zip_codes = [\"77002\"]  # test one ZIP code first\n",
    "\n",
    "async def scrape_one_zip(zip_code):\n",
    "    async with async_playwright() as p:\n",
    "        # Launch headless Firefox\n",
    "        browser = await p.firefox.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        # Open TDLR search page\n",
    "        await page.goto(\"https://www.tdlr.texas.gov/tools_search/\")\n",
    "        \n",
    "        # Fill the ZIP code field\n",
    "        await page.fill(\"#zipcodedata\", zip_code)\n",
    "        \n",
    "        # Click the search button and wait for the results page\n",
    "        async with page.expect_navigation(timeout=30000):\n",
    "            await page.click(\"#submit3\")\n",
    "        \n",
    "        # Now the results page is loaded\n",
    "        html = await page.content()\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Print snippet to confirm\n",
    "        print(soup.prettify()[:1000])\n",
    "        \n",
    "        await browser.close()\n",
    "\n",
    "# Run the function\n",
    "await scrape_one_zip(zip_codes[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1731c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted scraping function to return HTML\n",
    "async def scrape_one_zip(zip_code):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.firefox.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        await page.goto(\"https://www.tdlr.texas.gov/tools_search/\")\n",
    "        await page.fill(\"#zipcodedata\", zip_code)\n",
    "        \n",
    "        async with page.expect_navigation(timeout=30000):\n",
    "            await page.click(\"#submit3\")\n",
    "        \n",
    "        html = await page.content()  # return the full page HTML\n",
    "        await browser.close()\n",
    "        return html\n",
    "\n",
    "# Run for one ZIP code and get HTML\n",
    "html = await scrape_one_zip(\"77002\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e9edd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a685514e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a750b9f048d54df1a95ef24b320390b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping ZIP codes:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "\n",
    "# List of ZIP codes\n",
    "zip_codes = [\n",
    "    \"77002\",\"77021\",\"77040\",\"77059\",\"77078\",\"77098\",\"77384\",\"77479\",\"77547\",\n",
    "    \"77003\",\"77022\",\"77041\",\"77060\",\"77079\",\"77099\",\"77385\",\"77484\",\"77571\",\n",
    "    \"77004\",\"77023\",\"77042\",\"77061\",\"77080\",\"77204\",\"77386\",\"77489\",\"77573\",\n",
    "    \"77005\",\"77024\",\"77043\",\"77062\",\"77081\",\"77325\",\"77388\",\"77492\",\"77574\",\n",
    "    \"77006\",\"77025\",\"77044\",\"77063\",\"77082\",\"77336\",\"77389\",\"77493\",\"77578\"\n",
    "]\n",
    "\n",
    "# Function to scrape HTML for one ZIP code\n",
    "async def scrape_one_zip(zip_code):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.firefox.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        await page.goto(\"https://www.tdlr.texas.gov/tools_search/\")\n",
    "        await page.fill(\"#zipcodedata\", zip_code)\n",
    "        \n",
    "        async with page.expect_navigation(timeout=30000):\n",
    "            await page.click(\"#submit3\")\n",
    "        \n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "        return html\n",
    "\n",
    "# Function to parse HTML and extract companies\n",
    "def parse_companies(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    companies = []\n",
    "\n",
    "    # Extract Company Information tables\n",
    "    company_tables = soup.find_all(\"table\", attrs={\"border\": \"0\"})\n",
    "    for table in company_tables:\n",
    "        if table.find(text=\"Company Information:\"):\n",
    "            company_data = {}\n",
    "            rows = table.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cols = row.find_all(\"td\")\n",
    "                if len(cols) >= 2:\n",
    "                    left_text = cols[0].get_text(separator=\" \", strip=True)\n",
    "                    right_text = cols[1].get_text(separator=\" \", strip=True)\n",
    "                    if left_text.startswith(\"Name:\"):\n",
    "                        company_data[\"Company Name\"] = left_text.replace(\"Name:\", \"\").strip()\n",
    "                    elif left_text.startswith(\"DBA:\"):\n",
    "                        company_data[\"DBA\"] = left_text.replace(\"DBA:\", \"\").strip()\n",
    "                    elif left_text.startswith(\"Owner/Officer:\"):\n",
    "                        company_data[\"Owner/Officer\"] = left_text.replace(\"Owner/Officer:\", \"\").strip()\n",
    "                    elif left_text.startswith(\"Phone:\"):\n",
    "                        company_data[\"Phone\"] = left_text.replace(\"Phone:\", \"\").strip()\n",
    "            if company_data:  # only add if non-empty\n",
    "                companies.append(company_data)\n",
    "\n",
    "    # Extract Certificate Information table\n",
    "    cert_table = soup.find(\"table\", attrs={\"border\": \"1\"})\n",
    "    if cert_table and companies:\n",
    "        rows = cert_table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) == 2:\n",
    "                key = cols[0].get_text(strip=True)\n",
    "                value = cols[1].get_text(strip=True)\n",
    "                if \"Status\" in key:\n",
    "                    companies[0][\"Certificate Status\"] = value\n",
    "                elif \"Number\" in key:\n",
    "                    companies[0][\"Certificate Number\"] = value\n",
    "                elif \"Carrier Type\" in key:\n",
    "                    companies[0][\"Carrier Type\"] = value\n",
    "                elif \"Number of Active Tow Trucks\" in key:\n",
    "                    companies[0][\"Number of Active Tow Trucks\"] = value\n",
    "                elif \"Mailing\" in key:\n",
    "                    companies[0][\"Mailing Address\"] = value\n",
    "                elif \"Physical\" in key:\n",
    "                    companies[0][\"Physical Address\"] = value\n",
    "\n",
    "    return companies\n",
    "\n",
    "# Main async function to scrape all ZIP codes\n",
    "async def scrape_all_zipcodes(zip_codes):\n",
    "    all_companies = []\n",
    "    for zip_code in tqdm(zip_codes, desc=\"Scraping ZIP codes\"):\n",
    "        html = await scrape_one_zip(zip_code)\n",
    "        companies = parse_companies(html)\n",
    "        for company in companies:\n",
    "            company[\"ZIP Code Searched\"] = zip_code\n",
    "        all_companies.extend(companies)\n",
    "    return pd.DataFrame(all_companies)\n",
    "\n",
    "# Run the scraping and save to CSV\n",
    "df = await scrape_all_zipcodes(zip_codes)\n",
    "df.to_csv(\"tdlr_companies.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e84fc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV\n",
    "df.to_csv(\"tdlr_companies.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bddc1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/towcenter/Desktop/python/homework/homework-10\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8067a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
